# ğŸš€ Lakehouse Data Engineering Platform â€“ Databricks Community

## ğŸ“Œ Contexte du projet

Ce projet consiste en la conception et lâ€™implÃ©mentation dâ€™une **plateforme Data Engineering de type Lakehouse**, dÃ©veloppÃ©e en **PySpark** et dÃ©ployÃ©e sur **Databricks Community Edition**.  
Il sâ€™inscrit dans un cadre acadÃ©mique / capstone project et vise Ã  mettre en Å“uvre des **bonnes pratiques industrielles** de traitement de donnÃ©es Ã  grande Ã©chelle.

Le pipeline traite **plus de 8 Go de donnÃ©es**, issues de **sources hÃ©tÃ©rogÃ¨nes**, et repose sur une architecture **Bronze / Silver / Gold** garantissant :
- la traÃ§abilitÃ© des donnÃ©es,
- lâ€™idempotence des traitements,
- la qualitÃ© et la standardisation des datasets,
- lâ€™exploitabilitÃ© analytique et mÃ©tier.

---

## ğŸ—ï¸ Architecture globale

Lâ€™architecture du projet suit le **Lakehouse pattern** :


### ğŸ”¹ Bronze
- DonnÃ©es brutes ingÃ©rÃ©es sans transformation majeure
- Conservation maximale de lâ€™information
- TraÃ§abilitÃ© par `run_date` / `run_id`

### ğŸ”¹ Silver
- Nettoyage et standardisation
- SchÃ©mas explicites
- Gestion des valeurs nulles et doublons
- Harmonisation des types et formats
- Ã‰criture optimisÃ©e en **Parquet / Delta**

### ğŸ”¹ Gold
- DonnÃ©es agrÃ©gÃ©es et orientÃ©es mÃ©tier
- Tables analytiques prÃªtes pour la BI et le reporting
- Optimisation pour la performance et la lecture

---

## ğŸ“‚ Structure du projet


---

## ğŸ“Š Jeux de donnÃ©es utilisÃ©s

### 1ï¸âƒ£ Dataset principal â€“ Jane Street (anonymisÃ©)
- DonnÃ©es financiÃ¨res rÃ©alistes
- SÃ©ries temporelles non stationnaires
- CaractÃ©ristiques numÃ©riques Ã  haute dimension
- Cas dâ€™Ã©tude proche du trading algorithmique rÃ©el

### 2ï¸âƒ£ Dataset utilisateur â€“ Craigslist Cars & Trucks
Source : https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data

- DonnÃ©es semi-structurÃ©es
- ProblÃ¨mes de qualitÃ© (nulls, catÃ©gories incohÃ©rentes)
- Cas idÃ©al pour dÃ©montrer le nettoyage et la standardisation

---

## ğŸ” Idempotence du pipeline

Le pipeline est **entiÃ¨rement idempotent** :
- Relancer un job **ne corrompt jamais les rÃ©sultats**
- Ã‰criture partitionnÃ©e par `run_date`
- PossibilitÃ© dâ€™overwrite ciblÃ©
- CompatibilitÃ© avec exÃ©cutions incrÃ©mentales

---

## âœ… QualitÃ© des donnÃ©es

Des contrÃ´les systÃ©matiques sont appliquÃ©s :
- Comptage des valeurs nulles
- DÃ©tection des doublons
- VÃ©rification des schÃ©mas
- Statistiques descriptives
- Validation des clÃ©s de jointure

Des rapports de qualitÃ© sont gÃ©nÃ©rÃ©s dans `reports/data_quality/`.

---

## âš™ï¸ Technologies utilisÃ©es

- **Databricks Community Edition**
- **Apache Spark / PySpark**
- **Delta Lake**
- **Parquet**
- **Python**
- **Git & GitHub**
- **LaTeX** (rapport final)

---

## ğŸš€ ExÃ©cution du projet

1. Cloner le dÃ©pÃ´t GitHub dans Databricks (Repos)
2. Lancer les notebooks dâ€™ingestion (Bronze)
3. ExÃ©cuter les transformations Silver
4. GÃ©nÃ©rer les tables Gold
5. Consulter les rapports et agrÃ©gations finales

---

## ğŸ¯ Objectifs pÃ©dagogiques et techniques

- Appliquer une architecture Lakehouse complÃ¨te
- Manipuler de gros volumes de donnÃ©es
- Garantir la qualitÃ© et la fiabilitÃ© des pipelines
- Optimiser les performances Spark
- Approcher des cas rÃ©els de Data Engineering

---

## ğŸ“Œ Auteur

**Mohamed Dia**  
**NdiogOU DIADIE DIOUF
Data Engineering / Big Data  
Databricks Community Edition

---

## ğŸ“„ Licence

Projet Ã  vocation **acadÃ©mique et pÃ©dagogique**.  
Les datasets restent soumis Ã  leurs licences respectives.
